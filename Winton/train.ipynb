{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('./train.data.new.sum.csv')\n",
    "train_df = pd.read_csv('./train.data.new_prod.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_c7 = pd.read_csv('./c7.train.csv')\n",
    "# train_c7.c7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.Feature_7 = train_c7.c7_50\n",
    "train_df = pd.concat([train_df.Id, train_c7.c7, train_df.drop('Id', axis=1)], axis=1)\n",
    "train_df.Feature_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.c7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = train_df.columns[1:-64]\n",
    "feat_reg = r'Feature_|MOM|ALMA|OSMA|MACD|RSI|f7_c100|cumsum'\n",
    "# feature_list = train_df.filter(regex=feat_reg, axis=1).columns#[1:-62]\n",
    "# feature_list = train_df.filter(regex=r'sd|mean|Feature_7|MOM|MACD|OSMA', axis=1).columns #~0.61\n",
    "# feature_list = train_df.filter(regex=r'sd|mean|Feature|ALMA|EMA', axis=1).columns#[0:-62]\n",
    "# X = train_df.ix[:, indexes[0:476]]\n",
    "# X = train_df.ix[:, feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = pd.concat([pd.get_dummies(train_df['Feature_7']).rename(columns=lambda x: 'Feature_7_' + str(x)),\n",
    "#                            X], axis=1)\n",
    "# X.drop('Feature_7', axis=1, inplace=True)\n",
    "X = train_df.ix[:, feature_list]\n",
    "X.drop('Feature_7', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.head()\n",
    "# X.Feature_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input parametrs of training\n",
    "param_train_size = 0.7\n",
    "cv_param = {'max_depth': [16, 12],\n",
    "            'n_estimators': [250, 150],\n",
    "            'learning_rate': [0.15]}\n",
    "# suffix_file = \"_clf_all_features.pkl\"\n",
    "suffix_file = \"_clf_\" + feat_reg.replace('|', '_') + \".pkl\"\n",
    "y_list = ['Ret_PlusOne', 'Ret_PlusTwo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "median_neg = dict()\n",
    "median_pos = dict()\n",
    "X_trains = dict()\n",
    "X_tests = dict()\n",
    "y_trains = dict()\n",
    "y_tests = dict()\n",
    "X_valids = dict()\n",
    "y_valids = dict()\n",
    "clfs = dict()\n",
    "fitteds = dict()\n",
    "cut_dict = dict()\n",
    "xgb_models = dict()\n",
    "matrix_trains = dict()\n",
    "matrix_tests = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, train_df.Weight_Daily], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_ind = np.argmax(X.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "min_ind = np.argmin(X.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "\n",
    "X = pd.concat([pd.DataFrame(min_ind, columns=['min_ret']),\n",
    "               pd.DataFrame(max_ind, columns=['max_ret']),\n",
    "               X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = X.filter(regex=r'cumprod', axis=1).columns\n",
    "\n",
    "max_ind = np.argmax(X.ix[:, feature_list].as_matrix(), axis=1)\n",
    "min_ind = np.argmin(X.ix[:, feature_list].as_matrix(), axis=1)\n",
    "\n",
    "X = pd.concat([pd.DataFrame(min_ind, columns=['min_cum']),\n",
    "               pd.DataFrame(max_ind, columns=['max_cum']), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = X.filter(regex=r'cumprod', axis=1).columns\n",
    "\n",
    "std =    np.std(X.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "median = np.median(X.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "\n",
    "X = pd.concat([pd.DataFrame(std, columns=['std']),\n",
    "               pd.DataFrame(median, columns=['median']), X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X.drop('Weight_Daily', axis=1, inplace=True)\n",
    "# X = pd.concat([X, train_df.Weight_Intraday], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# X1 = StandardScaler().fit_transform(X.ix[:, :-121])\n",
    "# X = pd.DataFrame(X1, columns=X.columns[:-1])\n",
    "# X = pd.concat([X, train_df.Weight_Daily], axis=1)\n",
    "# Y  = train_df.filter(regex=\"MOM|MACD|cums\", axis=1)\n",
    "new_columns = [x + '_Scaler' for x in X.ix[:, :-1].columns.tolist()]\n",
    "scal = StandardScaler()\n",
    "X1 = scal.fit_transform(X.ix[:, :-1])\n",
    "X = pd.concat([pd.DataFrame(X1, columns=new_columns), \n",
    "              X], axis=1)\n",
    "# X.max_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X1 = X.copy()\n",
    "X = X1.copy()\n",
    "X = X.ix[:, l[0:950]]\n",
    "# y_tests[y_label]\n",
    "X.head()\n",
    "# X.Feature_7\n",
    "# len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_Dtrains = dict()\n",
    "X_Dvalids = dict()\n",
    "X_Dtests = dict()\n",
    "X_Dtrains_CL = dict()\n",
    "X_Dvalids_CL = dict()\n",
    "X_Dtests_CL = dict()\n",
    "X_Dtrains_CL_2 = dict()\n",
    "X_Dvalids_CL_2 = dict()\n",
    "X_Dtests_CL_2 = dict()\n",
    "xgb_models_cl = dict()\n",
    "xgb_models_cl_2 = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_list = ['Ret_PlusOne', 'Ret_PlusTwo']\n",
    "# y_list = ['Ret_121', 'Ret_170']\n",
    "for y_label in y_list:\n",
    "#     y_label = 'Ret_PlusOne'\n",
    "    median_neg[y_label] = np.median(train_df[y_label][train_df[y_label] < 0])\n",
    "    median_pos[y_label] = np.median(train_df[y_label][train_df[y_label] > 0])\n",
    "\n",
    "    cut_dict[y_label] = train_df[y_label]\n",
    "\n",
    "    X_trains[y_label], X_tests[y_label], y_trains[y_label], y_tests[y_label] = train_test_split(\n",
    "        X, cut_dict[y_label], train_size=param_train_size, random_state = 3124356)\n",
    "\n",
    "    \n",
    "    llen = X_tests[y_label].shape[0]/2\n",
    "    X_valids[y_label] = X_tests[y_label][0:llen]\n",
    "    X_tests[y_label] = X_tests[y_label][llen:]\n",
    "    y_valids[y_label] = y_tests[y_label][0:llen]\n",
    "    y_tests[y_label] = y_tests[y_label][llen:]\n",
    "#     X_Dtrains[y_label] = xgb.DMatrix(X_trains[y_label].ix[:, :-1],\n",
    "#                                        label=y_trains[y_label],\n",
    "#                                        weight=X_trains[y_label].ix[:, -1])\n",
    "    \n",
    "#     X_Dvalids[y_label] = xgb.DMatrix(X_valids[y_label].ix[:, :-1],\n",
    "#                                        label=y_valids[y_label],\n",
    "#                                        weight=X_valids[y_label].ix[:, -1])\n",
    "    \n",
    "#     X_Dtests[y_label] = xgb.DMatrix(X_tests[y_label].ix[:, :-1],\n",
    "#                                        label=y_tests[y_label],\n",
    "#                                        weight=X_tests[y_label].ix[:, -1])\n",
    "    \n",
    "    \n",
    "    X_Dtrains_CL[y_label] = xgb.DMatrix(X_trains[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_trains[y_label],\n",
    "                                           (-1, median_neg[y_label] * .7 , median_pos[y_label] * .7, 1),\n",
    "                                           labels=(0, 1, 2)),\n",
    "                                       weight=X_trains[y_label].ix[:, -1])\n",
    "    \n",
    "    X_Dvalids_CL[y_label] = xgb.DMatrix(X_valids[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_valids[y_label],\n",
    "                                           (-1, median_neg[y_label] * .7, median_pos[y_label] * .7, 1),\n",
    "                                           labels=(0, 1, 2)),\n",
    "                                       weight=X_valids[y_label].ix[:, -1])\n",
    "    \n",
    "    X_Dtests_CL[y_label] = xgb.DMatrix(X_tests[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_tests[y_label],\n",
    "                                           (-1, median_neg[y_label] * .7, median_pos[y_label] * .7, 1),\n",
    "                                           labels=(0, 1, 2)),\n",
    "                                       weight=X_tests[y_label].ix[:, -1])\n",
    "    \n",
    "    X_Dtrains_CL_2[y_label] = xgb.DMatrix(X_trains[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_trains[y_label],\n",
    "                                           (-1, 0, 1),\n",
    "                                           labels=(0, 1)),\n",
    "                                       weight=X_trains[y_label].ix[:, -1])\n",
    "    \n",
    "    X_Dvalids_CL_2[y_label] = xgb.DMatrix(X_valids[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_valids[y_label],\n",
    "                                           (-1, 0, 1),\n",
    "                                           labels=(0, 1)),\n",
    "                                       weight=X_valids[y_label].ix[:, -1])\n",
    "    \n",
    "    X_Dtests_CL_2[y_label] = xgb.DMatrix(X_tests[y_label].ix[:, :-1],\n",
    "                                       label=pd.cut(y_tests[y_label],\n",
    "                                           (-1, 0, 1),\n",
    "                                           labels=(0, 1)),\n",
    "                                       weight=X_tests[y_label].ix[:, -1])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def wmae(preds, dtrain):\n",
    "    target = dtrain.get_label()\n",
    "    weight = dtrain.get_weight()\n",
    "    x1 = np.abs(target - preds)\n",
    "    x2 = x1 * weight\n",
    "    x3 = np.mean(x2)\n",
    "    # Yes, this is needlessly complex, but I had debugging issues\n",
    "    return 'wmae', x3\n",
    "\n",
    "\n",
    "for y_label in y_list:\n",
    "\n",
    "#     watchlist = [(X_Dtrains[y_label], 'train'),\n",
    "#                  (X_Dvalids[y_label], 'valid'), \n",
    "#                  (X_Dtests[y_label], 'test')]\n",
    "    \n",
    "    param = {'max_depth':11, 'eta':0.04, 'silent':5}\n",
    "    param_CL = {'max_depth':11, 'eta':0.04,\n",
    "             'silent':5, 'eval_metric':'merror', 'objective':'multi:softprob',\n",
    "            'num_class':3}\n",
    "    num_round = 221\n",
    "    \n",
    "#     print 'valid', wmae(0, X_Dvalids[y_label]), 'test', wmae(0, X_Dtests[y_label]) \n",
    "    print(y_label)\n",
    "    print(str(datetime.datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "# #     regression\n",
    "                                       \n",
    "#     xgb_models[y_label] = xgb.train(param, X_Dtrains[y_label],\n",
    "#                                     num_boost_round=num_round,\n",
    "#                                     evals=[(X_Dtrains[y_label], 'train'),\n",
    "#                                      (X_Dvalids[y_label], 'valid'), \n",
    "#                                      (X_Dtests[y_label], 'test')],\n",
    "#                                     feval=wmae,\n",
    "#                                     early_stopping_rounds = 10)\n",
    "\n",
    "\n",
    "\n",
    "    xgb_models_cl[y_label] = xgb.train({'max_depth':16, 'eta':0.1,\n",
    "                             'silent':5, 'eval_metric':'merror', 'objective':'multi:softprob',\n",
    "                            'num_class':3},\n",
    "                                       X_Dtrains_CL[y_label],\n",
    "                                    num_boost_round=num_round,\n",
    "                                    evals=[(X_Dtrains_CL[y_label], 'train'),\n",
    "                                     (X_Dvalids_CL[y_label], 'valid'), \n",
    "                                     (X_Dtests_CL[y_label], 'test')],\n",
    "                                   early_stopping_rounds = 10)\n",
    "    \n",
    "    xgb_models_cl_2[y_label] = xgb.train({'max_depth':11, 'eta':0.04,\n",
    "             'silent':5, 'eval_metric':'merror', 'objective':'multi:softprob', 'num_class':2},\n",
    "             X_Dtrains_CL_2[y_label],\n",
    "             num_boost_round=num_round, \n",
    "             evals=[(X_Dtrains_CL_2[y_label], 'train'),\n",
    "              (X_Dvalids_CL_2[y_label], 'valid'), \n",
    "              (X_Dtests_CL_2[y_label], 'test')],\n",
    "               early_stopping_rounds = 10)\n",
    "    print(str(datetime.datetime.now()))\n",
    "    sys.stdout.flush()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(xgb_models[y_label], open('all_features_and_scales_all_featuresreg', \"wb\"))\n",
    "pickle.dump(xgb_models_cl_2[y_label], open('all_features_and_scales_all_features_cl_2', \"wb\"))\n",
    "pickle.dump(scal, open('scales_all_features', \"wb\"))\n",
    "\n",
    "# pickle.dump(xgb_models_cl[y_label], open('all_features_and_scales_all_features_cl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_importance(model):\n",
    "    importance = model.get_fscore()\n",
    "    tuples = [(k, importance[k]) for k in importance]\n",
    "    tuples = sorted(tuples, reverse=True, key=lambda x: x[1])\n",
    "    labels, values = zip(*tuples)\n",
    "    return labels, values\n",
    "y_label = 'Ret_PlusOne'\n",
    "l,v = get_importance(xgb_models_cl_2[y_label] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f = l[0:250]\n",
    "# v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_label = 'Ret_PlusOne'\n",
    "y_pred = xgb_models[y_label].predict(X_Dvalids[y_label])\n",
    "y_value = X_Dvalids[y_label].get_label()\n",
    "print y_pred, y_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_cl = xgb_models_cl[y_label].predict(X_Dvalids_CL[y_label])\n",
    "y_value_cl = X_Dvalids_CL[y_label].get_label()\n",
    "print y_pred_cl, y_value_cl\n",
    "print np.argmax(y_pred_cl, axis=1)\n",
    "cl = np.argmax(y_pred_cl, axis=1)\n",
    "prob_cl = np.max(y_pred_cl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_cl_2 = xgb_models_cl_2[y_label].predict(X_Dvalids_CL_2[y_label])\n",
    "y_value_cl_2 = X_Dvalids_CL_2[y_label].get_label()\n",
    "print y_pred_cl_2, y_value_cl_2\n",
    "print np.argmax(y_pred_cl_2, axis=1)\n",
    "cl_2 = np.argmax(y_pred_cl_2, axis=1)\n",
    "\n",
    "prob_cl_2 = np.max(y_pred_cl_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = cl.copy()\n",
    "y = y.astype(float64)\n",
    "y[(cl_2 == 0)] = -0.0002582153\n",
    "y[(cl_2 == 1)] = 0.0002\n",
    "print wmae(-0.000258483738816342, X_Dvalids_CL_2[y_label])\n",
    "print wmae(y, X_Dvalids_CL_2[y_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print wmae(0, X_Dvalids[y_label])\n",
    "print wmae(-0.000258483738816342, X_Dvalids[y_label])\n",
    "print wmae(y_pred, X_Dvalids[y_label])\n",
    "y_pred_copy = y_pred.copy()\n",
    "\n",
    "y_pred_copy[y_pred == y_pred] = 0\n",
    "condition_vec = (  ((cl_2 == 0) & (y_pred < 0)) | \n",
    "            ((cl_2 == 1) & (y_pred > 0)) )\n",
    "\n",
    "\n",
    "# condition_vec = (  (((cl_2 == 0) & (y_pred < 0)) | (y_cl == cl)) | \n",
    "#             (((cl_2 == 1) & (y_pred > 0)) | (y_cl == cl)) )\n",
    "y_pred_copy[condition_vec] = y_pred[condition_vec]\n",
    "\n",
    "# y_pred_copy[((cl == 0) & (y_pred > 0) & (cl_2 == 0)) | \n",
    "#             ((cl == 2) & (y_pred < 0) & (cl_2 == 1)) | \n",
    "#             ((cl == 1))] = 0\n",
    "# y_pred_copy[((cl == 0) & (y_pred > 0)) | \n",
    "#             ((cl == 2) & (y_pred < 0)) | \n",
    "#             ((cl == 1))] = 0\n",
    "print wmae(y_pred_copy, X_Dvalids[y_label])\n",
    "\n",
    "# ('wmae', 27709.582)\n",
    "# ('wmae', 27700.25)\n",
    "# ('wmae', 25733.922)\n",
    "# ('wmae', 25857.531)\n",
    "\n",
    "\n",
    "# most 800 features\n",
    "# ('wmae', 27709.582)\n",
    "# ('wmae', 27700.25)\n",
    "# ('wmae', 26655.352)\n",
    "# ('wmae', 26371.129)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_cl = pd.cut(y_pred,(-1, median_neg[y_label] * .7 , median_pos[y_label] * .7, 1),\n",
    "        labels=(0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(y_cl == cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_trains['Ret_PlusOne'].Feature_7\n",
    "sum((cl == 2) & (y_pred_copy > 0))\n",
    "print sum(((y_pred_copy > 0) & (y_value > 0))) + sum(((y_pred_copy < 0) & (y_value < 0)))\n",
    "print sum(((y_pred_copy < 0) & (y_value > 0))) + sum(((y_pred_copy > 0) & (y_value < 0)))\n",
    "\n",
    "print 2479. / (1414 + 2479) \n",
    "print sum(((y_pred > 0) & (y_value > 0))) + sum(((y_pred < 0) & (y_value < 0)))\n",
    "print sum(((y_pred < 0) & (y_value > 0))) + sum(((y_pred > 0) & (y_value < 0)))\n",
    "print 3518. / (3518 + 2482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = range(0, y_pred.shape[0])\n",
    "plt.figure(figsize=(70, 30))\n",
    "plt.scatter(t,\n",
    "            y_pred_copy,\n",
    "            s= 500*prob_cl_2,\n",
    "            c='green')\n",
    "\n",
    "plt.scatter(t,\n",
    "            y_value,\n",
    "            s= 100,\n",
    "            c='red')\n",
    "\n",
    "plt.axhline(0, color='black')\n",
    "plt.xlim(500, 3900)\n",
    "# plt.axhline(median_neg[y_label], color='black')\n",
    "# plt.axhline(median_pos[y_label], color='black')\n",
    "# plt.axhline(-0.021499999999917918, color='black')\n",
    "# plt.axhline(0.0254999999999, color='black')\n",
    "# print(mean(y_prob[:, 0] > .9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_models = dict()\n",
    "xgb_models_cl = dict()\n",
    "xgb_models_cl_2 = dict()\n",
    "y_label = 'Ret_PlusOne'\n",
    "# xgb_models[y_label] = dc['reg']\n",
    "# xgb_models_cl[y_label] = dc['cl']\n",
    "# xgb_models_cl_2[y_label] = dc['cl_2']\n",
    "\n",
    "xgb_models[y_label] = pickle.load(open('all_features_and_scales_all_featuresreg', \"rb\"))\n",
    "xgb_models_cl_2[y_label] = pickle.load(open('all_features_and_scales_all_features_cl_2', \"rb\"))\n",
    "scal = pickle.load(open('scales_all_features', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv('./test.data.new.sum.csv')\n",
    "test_df = pd.read_csv('./test.data.new_prod.csv')\n",
    "y_label = 'Ret_PlusOne'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_tests[y_label].Feature_7.head()\n",
    "\n",
    "test_c7 = pd.read_csv('./c7.test.csv')\n",
    "test_df.Feature_7 = test_c7.c7_50\n",
    "test_df['c7'] = test_c7.c7\n",
    "test_df.Feature_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_ind = np.argmax(test_df.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "min_ind = np.argmin(test_df.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "\n",
    "test_df = pd.concat([pd.DataFrame(min_ind, columns=['min_ret']),\n",
    "               pd.DataFrame(max_ind, columns=['max_ret']),\n",
    "               test_df], axis=1)\n",
    " \n",
    "feature_list = test_df.filter(regex=r'cumprod', axis=1).columns\n",
    "\n",
    "max_ind = np.argmax(test_df.ix[:, feature_list].as_matrix(), axis=1)\n",
    "min_ind = np.argmin(test_df.ix[:, feature_list].as_matrix(), axis=1)\n",
    "\n",
    "test_df = pd.concat([pd.DataFrame(min_ind, columns=['min_cum']),\n",
    "               pd.DataFrame(max_ind, columns=['max_cum']), test_df], axis=1)\n",
    "\n",
    "feature_list = test_df.filter(regex=r'cumprod', axis=1).columns\n",
    "\n",
    "std =    np.std(test_df.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "median = np.median(test_df.ix[:, -120:-1].as_matrix(), axis=1)\n",
    "\n",
    "test_df = pd.concat([pd.DataFrame(std, columns=['std']),\n",
    "               pd.DataFrame(median, columns=['median']), test_df], axis=1)\n",
    "\n",
    "test_df.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel median\n",
    "%xdel std\n",
    "%xdel min_ind\n",
    "%xdel max_ind\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_sub.head()\n",
    "# test_df.drop('Id', axis=1, inplace=True)\n",
    "# len(new_columns)\n",
    "# scal.transform(X_sub.ix[:, 1766:3520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = xgb_models[y_label].feature_names\n",
    "X_sub = test_df.ix[0:60000, feature_list[1760:3520]]\n",
    "# new_columns  = test_df.columns\n",
    "new_columns = [x + '_Scaler' for x in feature_list[1760:3520].tolist()]\n",
    "X_sub = pd.concat([pd.DataFrame(scal.transform(X_sub), columns=new_columns), \n",
    "              X_sub], axis=1)\n",
    "\n",
    "X_Dsub = xgb.DMatrix(X_sub.ix[:, feature_list])\n",
    "y_pred_0 = xgb_models[y_label].predict(X_Dsub)\n",
    "pickle.dump(y_pred_0, open('y_pred_0', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = xgb_models[y_label].feature_names\n",
    "X_sub = test_df.ix[60001:120000, feature_list[1760:3520]]\n",
    "# new_columns  = test_df.columns\n",
    "new_columns = [x + '_Scaler' for x in feature_list[1760:3520]]\n",
    "X_sub = pd.concat([pd.DataFrame(scal.transform(X_sub), columns=new_columns, index=X_sub.index), \n",
    "              X_sub], axis=1)\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "X_Dsub = xgb.DMatrix(X_sub.ix[:, feature_list])\n",
    "y_pred_1 = xgb_models[y_label].predict(X_Dsub)\n",
    "pickle.dump(y_pred_1, open('y_pred_1', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = xgb_models_cl_2[y_label].feature_names\n",
    "X_sub = test_df.ix[0:60000, feature_list[1760:3520]]\n",
    "# new_columns  = test_df.columns\n",
    "new_columns = [x + '_Scaler' for x in feature_list[1760:3520]]\n",
    "X_sub = pd.concat([pd.DataFrame(scal.transform(X_sub), columns=new_columns), \n",
    "              X_sub], axis=1)\n",
    "\n",
    "X_Dsub = xgb.DMatrix(X_sub.ix[:, feature_list])\n",
    "y_pred_0_CL_2 = xgb_models_cl_2[y_label].predict(X_Dsub)\n",
    "\n",
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "X_sub = test_df.ix[60001:120000, feature_list[1760:3520]]\n",
    "# new_columns  = test_df.columns\n",
    "X_sub = pd.concat([pd.DataFrame(scal.transform(X_sub), columns=new_columns, index=X_sub.index), \n",
    "              X_sub], axis=1)\n",
    "\n",
    "X_Dsub = xgb.DMatrix(X_sub.ix[:, feature_list])\n",
    "y_pred_1_CL_2 = xgb_models_cl_2[y_label].predict(X_Dsub)\n",
    "\n",
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(y_pred_0_CL_2, open('y_pred_0_CL_2', 'wb'))\n",
    "pickle.dump(y_pred_1_CL_2, open('y_pred_1_CL_2', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_list = xgb_models[y_label].feature_names\n",
    "# X_sub = test_df.ix[60001:120000, feature_list]\n",
    "# X_Dsub = xgb.DMatrix(X_sub)\n",
    "# y_pred_1 = xgb_models[y_label].predict(X_Dsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = xgb_models_cl[y_label].feature_names\n",
    "X_sub = test_df.ix[0:60000, feature_list]\n",
    "X_Dsub = xgb.DMatrix(X_sub)\n",
    "y_pred_0_CL = xgb_models_cl[y_label].predict(X_Dsub)\n",
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "gc.collect()\n",
    "X_sub = test_df.ix[60001:120000, feature_list]\n",
    "X_Dsub = xgb.DMatrix(X_sub)\n",
    "y_pred_1_CL = xgb_models_cl[y_label].predict(X_Dsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_list = xgb_models_cl_2[y_label].feature_names\n",
    "X_sub = test_df.ix[0:60000, feature_list]\n",
    "X_Dsub = xgb.DMatrix(X_sub)\n",
    "y_pred_0_CL_2 = xgb_models_cl_2[y_label].predict(X_Dsub)\n",
    "%xdel X_Dsub\n",
    "%xdel X_sub\n",
    "gc.collect()\n",
    "X_sub = test_df.ix[60001:120000, feature_list]\n",
    "X_Dsub = xgb.DMatrix(X_sub)\n",
    "y_pred_1_CL_2 = xgb_models_cl_2[y_label].predict(X_Dsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_0_CL_2 = pickle.load(open('y_pred_0_CL_2', 'rb'))\n",
    "y_pred_1_CL_2 = pickle.load(open('y_pred_1_CL_2', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_0 = pickle.load(open('y_pred_0', 'rb'))\n",
    "y_pred_1 = pickle.load(open('y_pred_1', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((y_pred_0, y_pred_1), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_copy = cl_2.copy()\n",
    "y_pred_copy = y_pred_copy.astype(float64)\n",
    "y_pred_copy[cl_2 == 0] = -0.000258483738816342\n",
    "y_pred_copy[cl_2 == 1] = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl_2 == 0\n",
    "y_pred_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred_CL = np.concatenate((y_pred_0_CL, y_pred_1_CL), axis = 0)\n",
    "y_pred_CL_2 = np.concatenate((y_pred_0_CL_2, y_pred_1_CL_2), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y_pred[abs(y_pred) > 0.2] = 0\n",
    "# sum(abs(y_pred) > 0.2)\n",
    "\n",
    "\n",
    "\n",
    "# cl = np.argmax(y_pred_CL, axis=1)\n",
    "cl_2 = np.argmax(y_pred_CL_2, axis=1)\n",
    "# prob_cl = np.max(y_pred_CL, axis=1)\n",
    "prob_cl_2 = np.max(y_pred_CL_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_copy = y_pred.copy()\n",
    "y_pred_copy[y_pred == y_pred] = 0\n",
    "print sum(y_pred_copy)\n",
    "condition_vec = (  ((cl_2 == 0) & (y_pred < 0)) | \n",
    "            ((cl_2 == 1) & (y_pred > 0)) )\n",
    "y_pred_copy[condition_vec] = y_pred[condition_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(y_pred_copy != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_copy.shape\n",
    "print sum(  ((cl == 0) & (y_pred > 0)) | \n",
    "            ((cl == 2) & (y_pred < 0)) | \n",
    "            ((cl == 1)) ) # <- 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sum(abs(y_pred_copy) > 0.1)\n",
    "y_pred_copy[abs(y_pred_copy) > 0.1] = 0\n",
    "print sum(abs(y_pred_copy) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_copy[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subm_df = pd.read_csv('./sample_submission_2.csv', header=0, index_col=0)\n",
    "s = (test_df.Id.astype(int64).astype(str) + '_61')\n",
    "subm_df.loc[s.as_matrix(), 'Predicted'] = y_pred_copy / 2\n",
    "# sum(subm_df.Predicted < 0)\n",
    "subm_df.to_csv('./cl2_all.features.standard.Scaler_median_div2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_label + suffix_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict_to_save = {\n",
    "#     'reg':xgb_models[y_label],\n",
    "#     'cl':xgb_models_cl[y_label],\n",
    "#     'cl_2':xgb_models_cl_2[y_label]\n",
    "# }\n",
    "# pickle.dump(dict_to_save, open(y_label + '_all_features', \"wb\"))\n",
    "pickle.dump(xgb_models[y_label], open('all_features_and_scales_all_features_reg', \"wb\"))\n",
    "# pickle.dump(xgb_models_cl_2[y_label], open('all_features_and_scales_all_features_cl_2', \"wb\"))\n",
    "# pickle.dump(xgb_models_cl[y_label], open('all_features_and_scales_all_features_cl', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dc = pickle.load(open('Ret_PlusOne_clf_Feature__OSMA_MOM_RSI_ALMA_Ret_.pkl', \"rb\"))\n",
    "# dc = pickle.load(open('Ret_PlusOne_clf_Feature__MOM_RSI_Ret_.pkl', \"rb\"))\n",
    "# dc = pickle.load(open('Ret_PlusOne_clf_Feature__MOM_ALMA_OSMA_MACD_RSI_f7_c100_cumsum.pkl', \"rb\"))\n",
    "dc = pickle.load(open('Ret_PlusOne_all_features', \"rb\"))\n",
    "dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_models = dict()\n",
    "xgb_models_cl = dict()\n",
    "xgb_models_cl_2 = dict()\n",
    "y_label = 'Ret_PlusOne'\n",
    "xgb_models[y_label] = dc['reg']\n",
    "xgb_models_cl[y_label] = dc['cl']\n",
    "xgb_models_cl_2[y_label] = dc['cl_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1000*prob_cl_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = range(0, y_pred.shape[0])\n",
    "plt.figure(figsize=(70, 30))\n",
    "plt.scatter(t,\n",
    "            y_pred_copy/10,\n",
    "            s= 1000*prob_cl_2,\n",
    "            c='green')\n",
    "plt.yticks(fontsize=36)\n",
    "\n",
    "plt.axhline(0, color='black')\n",
    "# plt.axhline(0.01, color='black')\n",
    "# plt.axhline(-0.010, color='black')\n",
    "plt.xlim(1000, 1900)\n",
    "plt.ylim(-0.001, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_cl_2 = np.max(y_pred_CL_2, axis=1)\n",
    "print y_cl_2, y_pred_CL_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sknn.mlp import Regressor, Layer\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = Regressor(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=250),\n",
    "        Layer(\"Sigmoid\", units=50),\n",
    "        Layer(\"Tanh\", units=20),\n",
    "        Layer(\"Sigmoid\", units=10),\n",
    "        Layer(\"Linear\")],\n",
    "    learning_rate=0.002,\n",
    "    n_iter=10)\n",
    "\n",
    "X_train = X_trains['Ret_PlusOne'].ix[:, :-1]\n",
    "\n",
    "# gs = GridSearchCV(nn, param_grid={\n",
    "#     'learning_rate': [0.002],\n",
    "#     'hidden0__units': [400],\n",
    "#     'hidden0__type': [\"Rectifier\", \"Tanh\", \"ExpLin\"],\n",
    "#     'hidden1__units': [800, 1200],\n",
    "#     'hidden1__type': [\"Sigmoid\", \"Tanh\"],\n",
    "#     'output__type': [\"Sigmoid\", \"Linear\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.fit(X_train.as_matrix(),\n",
    "       y_trains['Ret_PlusOne'].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_trains['Ret_PlusOne'].head() #.ix[:, :-1]\n",
    "# np.logspace(-2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = nn.predict(X_tests['Ret_PlusOne'].ix[:, :-1].as_matrix())\n",
    "y = y[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "y[y == y[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean(X_tests['Ret_PlusOne'].ix[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wmae(preds, target, weight):\n",
    "    x1 = np.abs(target - preds)\n",
    "    x2 = x1 * weight\n",
    "    x3 = np.mean(x2)\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print get_wmae(y, y_tests['Ret_PlusOne'], X_tests['Ret_PlusOne'].ix[:, -1])\n",
    "print get_wmae(0, y_tests['Ret_PlusOne'], X_tests['Ret_PlusOne'].ix[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tar = y_tests['Ret_PlusOne']\n",
    "# y_tar[y_tar < 0] = -0.0005\n",
    "# y_tar[y_tar > 0] = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = range(0, y.shape[0])\n",
    "plt.figure(figsize=(70, 30))\n",
    "plt.scatter(t,\n",
    "            y,\n",
    "            s = 150,\n",
    "            c='green')\n",
    "plt.scatter(t,\n",
    "            y_tar,\n",
    "            s = 150,\n",
    "            c='red')\n",
    "plt.yticks(fontsize=36)\n",
    "\n",
    "plt.axhline(0, color='black')\n",
    "# plt.axhline(0.01, color='black')\n",
    "# plt.axhline(-0.010, color='black')\n",
    "plt.xlim(1000, 1300)\n",
    "plt.ylim(-0.001, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X.ix[:, -120:-1].head().as_matrix()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
