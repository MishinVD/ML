{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Лабораторная работа 3. Линейные методы.\n",
    "\n",
    "Результат лабораторной работы − отчет. Мы предпочитаем принимать отчеты в формате ноутбуков IPython (ipynb-файл). Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете так же должен быть код, однако чем меньше кода, тем лучше всем: нам − меньше проверять, вам — проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после указанного срока сдачи нельзя. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, нам необходима ссылка на источник).\n",
    "\n",
    "Обратите внимание, что мы не ставим оценку за просто написанный код, корректная работоспособность которого не подтверждена экспериментами.\n",
    "\n",
    "### Правила сдачи\n",
    "Выполненную работу следует отправить в систему Anytask. Более подробно о системе можно почитать на странице курса. Название отправляемого файла должно иметь следующий формат: Surname_Name_Group_NN.ipynb, где NN — номер лабораторной работы. Например, Kozlova_Anna_CS_03.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логистическая регрессия\n",
    "\n",
    "В этом пункте мы будем рассматривать бинарную классификацию, где метки классов лежат во множестве $\\{-1, 1\\}$. \n",
    "\n",
    "Задачу обучения регуляризованной логистической регрессии можно записать следующим образом:\n",
    "\n",
    "$$ \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + \\exp(-\\langle w, x_i \\rangle y_i)) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Обучение данной модели сводится к нахождению параметров модели $w$, которое производится с помощью метода градиентного спуска. \n",
    "\n",
    "В данном случае градиентный шаг будет заключаться в обновлении вектора весов по следующей формуле:\n",
    "\n",
    "$$w := w + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-\\langle w, x_i \\rangle y_i)}\\Big) - \\eta Cw$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага.\n",
    "\n",
    "В общем случае метод градиентного спуска имеет некоторые недостатки:\n",
    "- попадание в локальные минимумы\n",
    "- неочевидность критерия останова\n",
    "- выбор размера шага\n",
    "- начальная инициализация весов\n",
    "\n",
    "В этой части лабораторной работы мы предложим вам реализовать метод градиентного спуска, а также рассмотрим некоторые его модификаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 1000 точек с 20 признаками на которой будете проводить эксперименты. Мы рекомендуем воспользоваться функцией [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification) из пакета sklearn. Обратите внимание, что метки классов для данной задачи должны быть из множества {-1, 1} (по умолчанию make_classification возвращает метки из множества {0, 1})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.5 балла)** Реализуйте градиентный спуск и протестируйте его для случая логистической регрессии на ранее сгенерированной выборке. Для сравнения качества разных подходов используйте значение оптимизируемого функционала.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше чем 1e-6\n",
    " - ограничить число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов можно сравнить следующие подходы:\n",
    " - нулева начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученный результат:\n",
    "- Рассмотрите как влияет размер шага на сходимость.\n",
    "- Сравните влияет ли наличие регуляризации на скорость сходимости и качество.\n",
    "- Постройте график качества оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту. \n",
    "\n",
    "**(1 балл)** Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов.\n",
    "\n",
    "- Посмотрите как влияет размер шага на сходимость\n",
    "- Постройте график качества оптимизируемого функционала в зависимости от номера итерации \n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между обновлением вектор весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окресности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{\\theta}{J(\\theta)}$$\n",
    "$$ \\theta = \\theta - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\theta$ — вектор параметров (в нашем случае — $w$)\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное его отличие в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $\\theta_i$ на итерации $t$ как $g_{t,i} = \\nabla_{\\theta}J(\\theta)$. \n",
    "\n",
    "В случае sgd обновление параметра $\\theta_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ номируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $\\theta_{i}$ до $t$-ой итерации. $\\varepsilon$ — параметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом ($\\odot$ — матричное умножение):\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична параметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $\\theta$:\n",
    "\n",
    "$$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta \\theta_t = - \\dfrac{RMS[\\Delta \\theta^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - оригинальная статья про [momentum](http://brahms.cpmc.columbia.edu/publications/momentum.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) про сравнение разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Реализуйте метод оптимизации *Momentum* **(0.5 балла)** и один из *Adagrad*/*Adadelta* **(1 балл)**. \n",
    "- Сравните с классическим sgd. \n",
    "- Посмотрите как значение параметра вдияет $\\gamma$ на скорость сходимости и качество в методе *Momentum*.\n",
    "- Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "![](http://66.147.244.197/~globerov/introspectivemode/wp-content/uploads/2012/08/regression-265x300.jpeg)\n",
    "\n",
    "Метод градиентного спуска позволяет оптимизировать произвольные функции. Например, рассмотрим задачу линейной регрессии, где $y \\in \\mathbb{R}$, а алгоритм будет иметь вид $a(x) = \\langle w, x\\rangle$. В случае метода наименьшиих квадратов оптимизируемый функционал можно записать следующим образом:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\langle w, x_i \\rangle - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "Эта задача интересна тем, что для нее можно выписать аналитическое решение. Попробуем сравнить два подхода."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 600 точек с двумя признаками для задачи регрессии, воспользовавшись функцией [make_regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Выпишите как выглядит точное решение задачи линейной регрессии. Решите задачу регрессии с помощью этого подхода без использования и с использованием регуляризации. Есть ли недостатки у такого подхода к решению задачи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Примените метод стохастического градиентного спуска из задания ранее. Сильно ли отличается полученный вектор параметров по сравнению с точным решением? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, линейная регрессия позволяет хорошо восстанавливать *линейные* зависимость, однако в общем случае хуже работает с более сложными данными. Это хорошо можно увидеть на следующем примере.\n",
    "\n",
    "Пусть исходная зависимость имеет вид $y = x \\cdot sin(x)$. Сгенерируем несколько точек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 100)\n",
    "y = X * np.sin(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKHS8eiwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b3563d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если теперь к полученным данным применить модель линейной регрессии, то получим следующее решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train[:, np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAEACAYAAAAA4lT2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ba59850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')\n",
    "y_plot = lr.predict(X[:, np.newaxis])\n",
    "_ = plt.plot(X, y_plot, c='r')\n",
    "_ = plt.legend(('linear regression', 'train', 'test'), bbox_to_anchor=(1.05, 1), loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели регрессии можно оценивать с помощью некоторых метрик, например $MSE = \\sum_{i=1}^l(a(x_i) - y_i)^2$ и в данном случае оно равно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.759977502823235"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lr.predict(X_test[:, np.newaxis])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенное решение совсем отдаленно напоминает изходную зависимость. Чтобы как-то исправить это одиним из подходов является добавление всех попарных произведений признаков, а также степеней: $x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(0.5 балла)** Воспользуйтесь классом [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) и добавьте к исходным данным различные полиномы.\n",
    "\n",
    " - рассмотрите как степень полинома (от 1 до 20) влияет на качество\n",
    " - изобразите на графике предсказание аналогично линейной регрессии\n",
    " - сравните этот подход с функцией [polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html)\n",
    "\n",
    "В чем могут быть недостатки такого подхода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный SVM\n",
    "\n",
    "Вернемся к задаче бинарной классификации. Будем обозначать обочающую выборку $\\{(x_n, y_n)\\}_{n=1}^N$, где $N$ — количество объектов, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — числовой вектор признакового описания объекта, $y_n \\in \\{+1, -1\\}$ — класс объекта.\n",
    "\n",
    "SVM обучает модель разделяющей гиперплоскости:\n",
    "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
    "Параметры модели — вектор весов $\\boldsymbol w \\in \\mathbb{R}^d$ и сдвиг $b \\in \\mathbb{R}$.\n",
    "\n",
    "Обучение модели происходит путем решения оптимизационной задачи:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
    "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Ограничения вида $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ требуют, чтобы объекты правильно классифицировались разделяющей гиперплоскостью. Поскольку линейная разделимость выборки не гарантируется на практике, вводят переменные $\\xi_n$ (slack variables), которые ослабляют ограничения правильной классификации. В оптимизируемом функционале слагаемое $\\| \\boldsymbol w \\|^2$ штрафует малую ширину разделяющей гиперплоскости, сумма $\\sum_n \\xi_n$ штрафует ослабление ограничений. \n",
    "\n",
    "После нахождения решения оптимизационной задачи $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, часть ограничений становятся _активными_, т.е. переходят в \"крайнее положение\" — точное равенство:\n",
    "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
    "Объекты, соответствующие активным ограничениям называются _опорными_.\n",
    "\n",
    "Гиперпараметр $C$ задает баланс между шириной разделяющей полосы и ошибками, допускаемыми классификатором. Обратите внимание, что $C$ фиксируется до обучения и не оптимизируется вместе с параметрами модели. Этот гиперпараметр отвечает за обобщающую способность разделяющей гиперплоскости, высокая обобщающая способность (соотвествующая большому значению $C$) может привести к переобучению, если линейная модель хорошо описывает обучающие примеры. При подборе оптимального параметра $C$ необходимо оценивать качество на отложенной выборке или кросс-валидации. Как правило, для конкретной задачи заранее неизвестно, какой порядок имеет оптимальное значение гиперпараметра $C$, поэтому перебирать значения лучше по логарифмической сетке, например: $10^{-3}, 10^{-2}, \\dots, 10^{5}$.\n",
    "\n",
    "Особенность этого метода в том, что он имеет решение, которое может быть найдено используя квадратичное прогаммирование. В этом задании мы не будем сводить данную задачу к задаче квадратичного программирования, а воспользуемся готовой реализацией из библиотеки sklearn.\n",
    "\n",
    "### Особенности реализации\n",
    "\n",
    "Обратите внимание, что в библиотеке sklearn можно найти 2 реализации линейного SVM: [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) и [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с ядровой функцией *linear*. Эти реализации имеют различие в библиотеках, на которых основаны: в первом случае используется библиотека *liblinear*, во втором — *libsvm*. Каждая из библотек имеет свои плюсы, поэтому перед применением стоит определиться какая из реализаций подходит больше. Обратите внимание, что это различие есть только для линейного SVM.\n",
    "\n",
    "В данном задании рекомендуем использовать класс [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с параметром *kernel='linear'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте три случайные двумерные выборки для бинарной классификации (хотя бы по 400 точек в каждой):\n",
    "- с линейно-разделимыми классами\n",
    "- с хорошо разделимыми классами, но не линейно\n",
    "- с плохо разделимыми классами по имеющимся признакам\n",
    "    \n",
    "Для генерации случайной выборки можно воспользоваться функциями, которые находятся в пакете [sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). Для того чтобы выборки не менялись при перезапуске ноутбука, фиксируйте параметр *random_state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Протестируйте линейный SVM  на сгенерированных выборках. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Как зависит число опорных векторов от параметра $C$ для различных выборок?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Явное преобразование признаков\n",
    "\n",
    "Как и в случае с линейной регрессией, когда оптимальныя разделяющая гиперплоскость не является линейной, данная модель является очень грубым решением. Линейная неразделимость векторов может быть исправлена путем перехода в другое признаковое пространство, в котором линейная модель лучше описывает данные и, возможно, существует правильно классифицирующая разделяющая гиперплоскость:\n",
    "\n",
    "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
    "\n",
    "Так, например, аналогичное добавление всех попарных произведений признаков: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ переводит в пространство, в котором линейная гиперплоскость является квадратичной формой в исходном пространстве и в исходном пространстве признаков разделяющая поверхность может быть, скажем, эллипсом.\n",
    "\n",
    "[Видеоролик с демонстрацией](https://youtu.be/9NrALgHFwTo)\n",
    "\n",
    "### Задание\n",
    "\n",
    "**(0.5 балла)** На тех же данных используя явное преобразование признаков обучите методом опорных векторов квадратичную разделяющую поверхность. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойственный переход и Ядровой SVM\n",
    "\n",
    "![](http://i.imgur.com/bJAzRCt.png)\n",
    "\n",
    "Задачу обучения линейного SVM, рассмотренную в предыдущем пункте принято называть _прямой_ оптимизационной задачей для SVM. Любая задача оптимизации с ограничениями имеет [_двойственную_ задачу Лагранжа](http://goo.gl/OujTPr), в которой оптимизируются _двойственные переменные_ (множители Лагранжа), соответствующие штрафу за нарушение ограничений, максимизируется нижняя оценка функционала прямой задачи. В случае задачи квадратичного программирования, решение двойственной задачи (значение оптимизируемого функционала) совпадает с оптимумом прямой задачи.\n",
    "\n",
    "Подробнее можно почитать в [статье](./SMAIS11_SVM.pdf).\n",
    "\n",
    "Двойственная задача для SVM имеет вид:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t. } \\quad  \n",
    "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
    "        & \\sum_{n} \\alpha_n y_n = 0\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Оптимизируется вектор из двойственных переменных $\\alpha_n$, соответствующих объектам обучающей выборки. Объект $x_n$ является опорным, если $\\alpha_n > 0$.\n",
    "\n",
    "Предсказание вычисляется по следующему правилу:\n",
    "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
    "\n",
    "Для предсказания необходимо оценить значение $b$. Известно, что для любого опорного объекта, который классифицируется безошибочно верно:\n",
    "$$y_n = \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'} + b,$$\n",
    "значит для любого такого объекта:\n",
    "$$b = y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}.$$\n",
    "\n",
    "В случае наличия ошибок классификации обучающей выборки, предлагается усреднять значение $b$ по всем опорным векторам:\n",
    "$$b = \\frac{1}{N_\\text{SV}}\\sum_{n \\in \\text{SV}}\\left(y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}\\right).$$\n",
    "Интуиция здесь такова, что суммарные ошибки в положительную сторону примерно равны суммарным ошибкам в отрицательную сторону.\n",
    "\n",
    "Другой вариант — отказаться от параметра $b$ и работать с моделью $f(x) = w^Tx$, добавив к вектору $x$ константный признак.\n",
    "\n",
    "#### Неявное преобразование признаков\n",
    "Отметим, что двойственная задача SVM содержит вектора признаков исключительно в виде скалярного произведения $x^Tx'$. Эта особенность позволяет производить неявное преобразование признакового пространства. Вместо вычисления функции $\\phi(\\boldsymbol x)$, которая может отображать исходные признаки в вектора очень большой размерности, будем вычислять скалярное произведение $k(\\boldsymbol x, \\boldsymbol x') = \\phi(\\boldsymbol x)^T\\phi(\\boldsymbol x')$ называемое _ядром_. \n",
    "\n",
    "\n",
    "В этом задании используйте класс $sklearn.svm.SVC$, меняя тип ядра. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(1 балл)** Протестируйте на предыдущих двумерных выборках ядровой SVM. Покажите на плоскости строящиеся разделяющие поверхности, линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Попробуйте следующие ядровые функции:\n",
    "- линейная: $k(x, x') = x^Tx'$\n",
    "- полиномиальная: $k(x, x') = (x^Tx' + 1)^d$ с различными степенями $d = 2,3,\\dots$\n",
    "- Гауссовская-RBF: $k(x, x') = \\exp(-\\sigma\\|x - x'\\|^2)$\n",
    "\n",
    "Ответьте на следующие вопросы:\n",
    " - Как ведет себя SVM с полиномиальным ядром в зависимости от параметров $C$ и степени ядра $d$?\n",
    " - Как ведет себя SVM с RBF-ядром в зависимости от параметров $C$ и $\\sigma$? Поварьируйте параметры $C$ и $\\sigma$ по логарифмической сетке. Какие значения параметров ведут к переобучению, а какие — к слишком грубой модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "**(1 балл)** В этой работе вы рассмотрели некоторые линейные модели машинного обучения, а также способы их обучения. Ответьте на следующие вопросы:\n",
    "\n",
    " - Какие есть достоинства у рассмотренных моделей? Дайте свой ответ для каждой модели.\n",
    " - Каким общим недостатком обладают данные модели? Какие есть способы его устранения? В чем может заключаться сложность использования этих подходов?\n",
    " - В чем заключаются различия с точки зрения обучения алгоритмов? Какие есть достоинства и недостатки у рассмотренных методов обучения?\n",
    " - Предположите в каком случае каждый из алгоритмов будет работать лучше: при большом/небольшом количестве данных? Поясните почему. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
